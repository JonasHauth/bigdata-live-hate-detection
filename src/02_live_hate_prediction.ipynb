{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Local Spark Environment (3.3.1)\n",
    "- 4 Cores, 8 Threats (1.8 Ghz Base Clock)\n",
    "- 8 GB RAM (shared with OS)\n",
    "- SSD "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durchlaufen der Systems der Daten\n",
    "1. Einlesen der Daten\n",
    "2. Erstellen eines Lesestroms f√ºr die Daten, mit maximal 2 Dateien pro Trigger\n",
    "3. Durchlaufen der Interferenzpipeline\n",
    "\n",
    "Pfade:\n",
    "Memory-bound: Bearbeiten der Daten im Zwischenspeicher (RAM)\n",
    "CPU-bound: Mit allen Daten im RAM sind wir CPU-Bound\n",
    "\n",
    "Skalierung: \n",
    "Unsere¬†Anwendung ist noch nicht f√ºr die Verarbeitung von Big Data skaliert, da sie auf einem einzelnen lokalen System ausgef√ºhrt wird (Local Mode). Erst wenn die Menge an Daten, gr√∂√üer als der verf√ºgbare Arbeitsspeicher des lokalen Systems ist, werden die Leistungsprobleme und Out-of-Memory-Fehler auftreten.\n",
    "\n",
    "Was passiert wenn die Datenmengen gr√∂√üenordnungsm√§√üig zunehmen? (1x/10x/100x... oder 2x/4x/8x/16x/...)?\n",
    "- Erh√∂hte Verarbeitungsleistung: Das Upscaling der Live-Analyse erfordert in der Regel eine h√∂here Verarbeitungsleistung, da mehr Daten in Echtzeit analysiert werden m√ºssen. Dies f√ºhrt zu dem Einsatz von mehr oder leistungsf√§higerer Hardware. Hierbei ist zu beachten dass eine horizontale Skalierung effizienter ist.\n",
    "- Erh√∂hte Speicherkapazit√§t: Das Upscaling der Live-Analyse erfordert auch mehr Speicherkapazit√§t, um die zus√§tzlichen Daten zu speichern, die analysiert werden. Dies k√∂nnte die Verwendung gr√∂√üerer Festplatten oder zus√§tzlicher Speichersysteme bedeuten.\n",
    "- Erh√∂hte Bandbreite: Die Hochskalierung der Live-Feed-Datenanalyse erfordert m√∂glicherweise mehr Bandbreite f√ºr die √úbertragung der zus√§tzlichen Daten, die analysiert werden. Dies kann zu einem Problem werden, wenn die Netzwerkinfrastruktur nicht in der Lage ist, die erh√∂hte Last zu bew√§ltigen.\n",
    "\n",
    "Was passiert wenn die Anfragen oder Abfragen gr√∂√üenordnungsm√§√üig zunehmen oder die Latenzbedingungen gr√∂√üenordnungsm√§√üig abnehmen?\n",
    "- Erh√∂hte Verarbeitungszeit: Wenn die Anfragen oder Abfragen gr√∂√üer werden, kann ihre Verarbeitung l√§nger dauern, da mehr Daten zu analysieren sind. Dies kann zu erh√∂hten Latenzzeiten und einer insgesamt langsameren Leistung f√ºhren.\n",
    "- Erh√∂hte Ressourcennutzung: Gr√∂√üere Anfragen oder Abfragen erfordern unter Umst√§nden auch mehr Ressourcen, z. B. Verarbeitungsleistung, Arbeitsspeicher und Speicherplatz, um verarbeitet zu werden. Dies kann zu einer erh√∂hten Ressourcennutzung f√ºhren und m√∂glicherweise die Leistung anderer Aufgaben oder Prozesse, die gleichzeitig laufen, beeintr√§chtigen.\n",
    "- Geringere Leistung: Wenn die Latenzbedingungen abnehmen, kann es schwieriger werden, Anfragen oder Abfragen in Echtzeit zu verarbeiten, da weniger Zeit f√ºr die Verarbeitung der Daten zur Verf√ºgung steht. Dies kann zu einer verringerten Leistung und m√∂glicherweise zu verpassten Analysem√∂glichkeiten f√ºhren.\n",
    "- Erh√∂hte Komplexit√§t: Sowohl gr√∂√üere Anfragen oder Abfragen als auch geringere Latenzzeiten k√∂nnen die Datenanalyse komplexer machen, da mehr Daten zu verarbeiten sind und weniger Zeit zur Verf√ºgung steht. Dies kann den Einsatz fortschrittlicherer Algorithmen oder Techniken erforderlich machen, um das gr√∂√üere Datenvolumen zu bew√§ltigen und die Echtzeitverarbeitungsanforderungen zu erf√ºllen.\n",
    "\n",
    "Welche Pfade sind einfach/schwieriger Skalierbar?\n",
    "- Datenerfassung: Einfache Skalierbarkeit, da weitere Server hinzugef√ºgt werden k√∂nnen, um die erh√∂hte Last zu bew√§ltigen und mehr Daten von der Twitter-API zu sammeln.\n",
    "- Datenbereinigung: Aktueller Prozess zur Datenbereinigung relativ simpel (entfernen von Sonderzeichen, Kleinschreibung, etc.) somit kann der Pfad gut skalieren.\n",
    "- Datenumwandlung: Aktuell ist die Datenumwandlung noch simpel, und das System kann gut skalieren. Falls die Datenumwandlung¬† jedoch komplexer werden sollte (Maschine Learning, Erkennen von Text in Bildern/Audio konvertieren in Text, etc.). w√ºrden w√ºrde das die Skalierung erheblich schwieriger machen.\n",
    "- Modellinterferenz: Skaliert gut, da nur ein begrenzter Datensatz f√ºr das Training genommen werden kann, welcher gelabelt wurde. \n",
    "- Datenspeicherung: Dieser Schritt ist relativ einfach zu skalieren, da weitere Server hinzugef√ºgt werden k√∂nnen, um die erh√∂hte Last zu bew√§ltigen, und die Daten auf die Server verteilt werden k√∂nnen.\n",
    "\n",
    "Dimensionen Zielsystem Prototyp:\n",
    "- Speicheranforderungen: Speichern Nur Text: 15 GB/Tag -> = 5 TB/Jahr\n",
    "Redundanz -> RF = 3 -> = 15 TB /Jahr -> 8 Festplatten a 2 TB -> 1¬†Unit\n",
    "- CPU Anforderungen: 4 Kerne mit 8 Threats (1.8 Ghz Base Clock) = 2000 Tweets/ Sekunde\n",
    "H√∂chstleistung = 10 000 Tweets / Sekunde\n",
    "Abdeckung von Systemausf√§llen und Anstieg der Nutzer = 20 000 Tweets/ Sekunden -> 40 Kerne mit 8 Threats (1.8 Ghz Base Clock)\n",
    "\n",
    "Dimensionen Zielsystem kommerzielles System:\n",
    "- CPU Anforderungen\n",
    "Prototyp: 4 Kerne mit 8 Threats (1.8 Ghz Base Clock) = 2000 Tweets/ Sekunde\n",
    "Anforderung: Durchschnitt 6000 Tweets/ Sekunde\n",
    "H√∂chster Messtand: 10 000 Tweets/ Sekunde\n",
    "Abdeckung von Systemaus√§llen = 20 000 Tweets/ Sekunden\n",
    "Weiterentwicklung des Systems (Analyse von Videos, Bilder, andere Sprachen, etc.)\n",
    "->x25 Prototypleistung Leistung\n",
    "1000 Kerne mit 8 Threats (1.8 Ghz Base Clock)\n",
    "- Speicher Anforderung\n",
    "Twitter Daten Upload 12 TB/Tag\n",
    "->4.5 PB/Jahr\n",
    "45Unit x 8 Festplatten (2TB) = 720 TB\n",
    " -> 3 Racks a¬†45 Units f√ºr 1 Jahr\n",
    "\n",
    "\n",
    "Fehler in der Analyse: \n",
    "-  Implementierung von Mechanismen zur Behandlung von Ausnahmen im Code, z. B. Try-Except-Bl√∂cke\n",
    "-  Implementierung eines Meldesystems und die manuelle √úberpr√ºfung der Fehler k√∂nnen die Falsch-Positiv sowie Falsch-Negativen Ereignisse behoben werden\n",
    "\n",
    "Fehlertoleranz verteiltes System: \n",
    "- DataFrame wird zur Fehlertoleranz √ºber mehrere Knoten in einem Cluster repliziert. Dies tr√§gt zur Fehlertoleranz im Falle von Knotenausf√§llen bei. Die verlorenen Daten k√∂nnen von den Replikaten wiederhergestellt werden.\n",
    "- Lineage Graph zur um verlorene Daten wiederherzustellen\n",
    "\n",
    "Daten Speicherung:\n",
    "- Twitter verwendet aktuell Hadoop-Cluster, auf denen Datenverarbeitung und HDFS laufen \n",
    " + Einfache Anbindung, Synergien\n",
    " - Geschwindigkeit ausreichend f√ºr Realtime analyse?\n",
    "\n",
    "Daten Codieung:\n",
    "- Unstrukturierte Daten, wie der Twitter Feed und Text-Dateien, k√∂nnen nicht direkt im Parquet-Format gespeichert werden. Es gibt jedoch die M√∂glichkeit, unstrukturierte Daten zu kodieren oder zu transformieren, um sie in Parquet zu speichern.\n",
    "- Daten werden in ein strukturiertes Format konvertieren (JSON), dieses wird dann in Parquet gespeichert \n",
    "- Prozess der Kodierung:\n",
    "    1. Serialisierung: Die Twitter-Feed-Daten m√ºssen in ein Format serialisiert werden, das auf die Festplatte geschrieben werden kann.\n",
    "    2. Auf die Festplatte schreiben: Die serialisierten Twitter-Feed-Daten k√∂nnen unter Verwendung des Parquet-Dateiformats auf die Festplatte geschrieben werden.\n",
    "    3. Von der Festplatte lesen: Die serialisierten Twitter-Feed-Daten k√∂nnen mit Hilfe der PySpark-API von der Festplatte zur√ºckgelesen werden.\n",
    "    4. Deserialisierung: Die Twitter-Feed-Daten k√∂nnen wieder in ihr urspr√ºngliches Format deserialisiert werden. Dies kann mit derselben Bibliothek geschehen, die f√ºr die Serialisierung verwendet wurde.\n",
    "\n",
    "Abk√ºrzungen des Prototyps:\n",
    "- Limitierte Integration in Systemlandschaft\n",
    "- Keine Analye von Videos, Bildern und Tonspuren\n",
    "- Sentiment Analyse nur auf Englisch\n",
    "- Tainingsdatensatz ist limietiert.\n",
    "\n",
    "Echtzeitf√§higkeit des Prototyps:\n",
    "Durch 4 Kerne mit 8 Threats (1.8 Ghz Base Clock) f√ºhrt zu einem Durchsatz von 2000 Tweets/Sekunde. Dies stellt bereits einen erheblichen Anteil dar, wenn man dies mit den durchschnittlichen Menge von 6000 Tweets/ Sekunde vergleicht. Jedoch handelt es sich hierbei um Tweets in reiner Textform. F√ºr eine Live-Analyse, mit Metadaten sowie Tweets welche Bilder und Ton beinhalten,m√ºsste das System ausgebaut werden. Dies k√∂nnte bspw. anhand von Spark Streaming mit einer horizontalen Skalierung durchgef√ºhrt werden.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version:  3.3.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Jonas-Surface:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>multi_class_text_classifiter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x206465f7070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col, regexp_replace, lower\n",
    "import os\n",
    "\n",
    "\n",
    "MAX_MEMORY = \"6g\"\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('live_hate_prediction')\\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "                    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.1\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Source\n",
    "Als Streaming-Quelle f√ºr den Prototyp nutzen wir das lokale Filesystem. Hier haben wir ca. 4 Mio. ungesehene Tweets, welche auf Hate-Nachrichten √ºberpr√ºft werden sollen. \n",
    "Daf√ºr sind die Tweets mit Metadaten im Parquet-Format gespeichert (ca 5 GB Rohdaten).\n",
    "Folgende Schritte sind hierf√ºr notwendig:\n",
    "- Einlesen des Daten-Schemas\n",
    "- Erstellen eines ReadStreams auf den Daten (die maximale Anzahl der Files pro Trigger wird auf zwei begrenzt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Anzahl der Partitions: 10\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.parquet('C://data/twitter-stream-parquet-medium/')\n",
    "dataSchema = spark_df.schema\n",
    "\n",
    "print(f\" Anzahl der Partitions: {spark_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 2).parquet('C://data/twitter-stream-parquet-medium/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferenz Pipeline laden\n",
    "- Die zuvor erstelle Inferenzpipeline wird in den Hauptspeichergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipeline = PipelineModel.load(\"../models/mllib_model_nb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Die Daten werden in Batches unterteilt und jeder Batch wird einzeln und parallel mit den Sparkoperationen verarbeitet. Die Ergebnisse der einzelnen Batches werden zum Endergebnis kombiniert. \n",
    "\n",
    "Folgende Methoden werden auf den Daten ausgef√ºhrt:\n",
    "- Submethode clean_text wird auf jedem Micro-Batch ausgef√ºhrt und bereinigt die Texte (z.B. Gro√ü- und Kleinschreibung, Sondernzeichen, ...)\n",
    "- Methode batch_hate_interference wird auf jedem Micro-Batch ausgef√ºhrt und ruft Submethode clean_text auf, f√ºhrt die Inferenz-Pipeline aus und speichert die identifizierten Hate-Tweets zur weiteren Verarbeitung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\") # Remove links\n",
    "  c = regexp_replace(c, \"(\\\\n)|\\n|\\r|\\t\", \"\") # Remove CR, tab, and LR\n",
    "  c = regexp_replace(c, \"(?:(?:[0-9]{2}[:\\/,]){2}[0-9]{2,4})\", \"\") # Remove dates\n",
    "  c = regexp_replace(c, \"@([A-Za-z0-9_]+)\", \"\") # Remove usernames\n",
    "  c = regexp_replace(c, \"[0-9]\", \"\") # Remove numbers\n",
    "  c = regexp_replace(c, \"\\:|\\/|\\#|\\.|\\?|\\!|\\&|\\\"|\\,\", \"\") # Remove symbols\n",
    "  return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_hate_inference(df, test):\n",
    "\n",
    "    print(f'Iter: {test}')\n",
    "\n",
    "    # Clean Text Columns\n",
    "    df = df.withColumn(\"text\", clean_text(col(\"text\")))\n",
    "\n",
    "    # Predict on Batch\n",
    "    final_df = trained_pipeline.transform(df).select('text','prediction')\n",
    "\n",
    "    # Handling for data (Save Hate Speech)\n",
    "    hate = final_df.filter(final_df['prediction'] == 1.0)\n",
    "    hate.write.mode('append').parquet('../data/parquet_output')\n",
    "    times_hate = hate.count()\n",
    "    print(f'Times hate detected: {times_hate}')\n",
    "\n",
    "    not_hate = final_df.filter(final_df['prediction'] == 0.0)\n",
    "    times_not_hate = not_hate.count()\n",
    "    print(f'Times not hate detected: {times_not_hate}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Streaming-Methoden Foreach und ForeachBatch erm√∂glichen, dass eigene Logik auf den Stream ausgef√ºhrt werden kann. \n",
    "    - Trigger: availableNow ist nur f√ºr den Demo-Modus, beendet den Stream nachdem alle Verf√ºgbaren Daten abgearbeitet wurden.\n",
    "    - Start: Beginnt Ausf√ºhrung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Times hate detected: 202\n",
      "Times not hate detected: 4920\n",
      "Iter: 1\n",
      "Times hate detected: 204\n",
      "Times not hate detected: 4686\n",
      "Iter: 2\n",
      "Times hate detected: 209\n",
      "Times not hate detected: 4876\n",
      "Iter: 3\n",
      "Times hate detected: 223\n",
      "Times not hate detected: 4987\n",
      "Iter: 4\n",
      "Times hate detected: 181\n",
      "Times not hate detected: 4632\n"
     ]
    }
   ],
   "source": [
    "batchHateInference = streaming.writeStream.foreachBatch(batch_hate_inference).trigger(availableNow=True).start() #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten zu Diagnose und Protokoll der Ausf√ºhrung:\n",
    "- Mit dem aktuellen Setup (lokales Spark) k√∂nnen ca. 2000 Tweets / Sekunde untersucht werden.\n",
    "- Durch Spark Streaming kann der Workload horizontal skaliert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '8ff70e78-1df3-4525-9d3b-8fcdbdb6512f',\n",
       "  'runId': '63858400-bdc1-4521-b96c-67bc42663c92',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T14:56:33.731Z',\n",
       "  'batchId': 0,\n",
       "  'numInputRows': 15366,\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'processedRowsPerSecond': 1895.1652688702516,\n",
       "  'durationMs': {'addBatch': 6601,\n",
       "   'getBatch': 88,\n",
       "   'latestOffset': 440,\n",
       "   'queryPlanning': 33,\n",
       "   'triggerExecution': 8108,\n",
       "   'walCommit': 396},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': None,\n",
       "    'endOffset': {'logOffset': 0},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 15366,\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'processedRowsPerSecond': 1895.1652688702516}],\n",
       "  'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}},\n",
       " {'id': '8ff70e78-1df3-4525-9d3b-8fcdbdb6512f',\n",
       "  'runId': '63858400-bdc1-4521-b96c-67bc42663c92',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T14:56:41.840Z',\n",
       "  'batchId': 1,\n",
       "  'numInputRows': 14670,\n",
       "  'inputRowsPerSecond': 1809.1009988901221,\n",
       "  'processedRowsPerSecond': 2156.401587534911,\n",
       "  'durationMs': {'addBatch': 5665,\n",
       "   'getBatch': 63,\n",
       "   'latestOffset': 343,\n",
       "   'queryPlanning': 5,\n",
       "   'triggerExecution': 6803,\n",
       "   'walCommit': 327},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 0},\n",
       "    'endOffset': {'logOffset': 1},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 14670,\n",
       "    'inputRowsPerSecond': 1809.1009988901221,\n",
       "    'processedRowsPerSecond': 2156.401587534911}],\n",
       "  'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}},\n",
       " {'id': '8ff70e78-1df3-4525-9d3b-8fcdbdb6512f',\n",
       "  'runId': '63858400-bdc1-4521-b96c-67bc42663c92',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T14:56:48.643Z',\n",
       "  'batchId': 2,\n",
       "  'numInputRows': 15255,\n",
       "  'inputRowsPerSecond': 2242.393061884463,\n",
       "  'processedRowsPerSecond': 2236.1477572559365,\n",
       "  'durationMs': {'addBatch': 5630,\n",
       "   'getBatch': 64,\n",
       "   'latestOffset': 361,\n",
       "   'queryPlanning': 5,\n",
       "   'triggerExecution': 6822,\n",
       "   'walCommit': 324},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 1},\n",
       "    'endOffset': {'logOffset': 2},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 15255,\n",
       "    'inputRowsPerSecond': 2242.393061884463,\n",
       "    'processedRowsPerSecond': 2236.1477572559365}],\n",
       "  'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}},\n",
       " {'id': '8ff70e78-1df3-4525-9d3b-8fcdbdb6512f',\n",
       "  'runId': '63858400-bdc1-4521-b96c-67bc42663c92',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T14:56:55.466Z',\n",
       "  'batchId': 3,\n",
       "  'numInputRows': 15630,\n",
       "  'inputRowsPerSecond': 2290.7811812985487,\n",
       "  'processedRowsPerSecond': 2330.0536672629696,\n",
       "  'durationMs': {'addBatch': 5540,\n",
       "   'getBatch': 64,\n",
       "   'latestOffset': 378,\n",
       "   'queryPlanning': 7,\n",
       "   'triggerExecution': 6708,\n",
       "   'walCommit': 335},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 2},\n",
       "    'endOffset': {'logOffset': 3},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 15630,\n",
       "    'inputRowsPerSecond': 2290.7811812985487,\n",
       "    'processedRowsPerSecond': 2330.0536672629696}],\n",
       "  'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}},\n",
       " {'id': '8ff70e78-1df3-4525-9d3b-8fcdbdb6512f',\n",
       "  'runId': '63858400-bdc1-4521-b96c-67bc42663c92',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T14:57:02.175Z',\n",
       "  'batchId': 4,\n",
       "  'numInputRows': 14439,\n",
       "  'inputRowsPerSecond': 2152.183633924579,\n",
       "  'processedRowsPerSecond': 2000.4156275976725,\n",
       "  'durationMs': {'addBatch': 5929,\n",
       "   'getBatch': 66,\n",
       "   'latestOffset': 365,\n",
       "   'queryPlanning': 4,\n",
       "   'triggerExecution': 7218,\n",
       "   'walCommit': 343},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 3},\n",
       "    'endOffset': {'logOffset': 4},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 14439,\n",
       "    'inputRowsPerSecond': 2152.183633924579,\n",
       "    'processedRowsPerSecond': 2000.4156275976725}],\n",
       "  'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchHateInference.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchHateInference.status"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende Tweets wurden durch das Modell als Hate-Speech identifiziert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rt  a power top and a perfect cumdump  it's al...\n",
       "1                rt  le cogi un amor cb a estar en casa ü•∞\n",
       "2       rt  in the conversation about student loans on...\n",
       "3       nossa vc fica nervosa com tudo  claro n√£o tenh...\n",
       "4       bad guys was such a funny movie i highly recom...\n",
       "                              ...                        \n",
       "2033                                         rt  tan tit \n",
       "2034    rt  amei te conhecer  vc me ensinou mta coisa ...\n",
       "2035    rt  raise your hand if you think antifa should...\n",
       "2036                                              natasha\n",
       "2037      fair enough at least you‚Äôre honest about hav...\n",
       "Name: text, Length: 2038, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_hate_df = spark.read.parquet('../data/parquet_output')\n",
    "hate_texts = detected_hate_df.select('text').toPandas()['text']\n",
    "hate_texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Anzahl der Partitions: 10\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.parquet('C://data/twitter-stream-parquet-medium/')\n",
    "dataSchema = spark_df.schema\n",
    "\n",
    "print(f\" Anzahl der Partitions: {spark_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 2).parquet('C://data/twitter-stream-parquet-medium/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_hate_inference(row):\n",
    "\n",
    "    # text = row[['text']]\n",
    "    \n",
    "    df = row.toDF()\n",
    "    df.write.mode('append').parquet('../data/parquet_live_output')\n",
    "\n",
    "    #spark.write.mode('append').parquet('../data/hate_detected')\n",
    "\n",
    "    # df = df.withColumn(\"text\", clean_text(col(\"text\")))\n",
    "\n",
    "    # wordsDataFrame_transformed = prep_trained_pipeline.transform(df)\n",
    "    # final_df = model_trained_pipeline.transform(wordsDataFrame_transformed).select('text','prediction')\n",
    "    \n",
    "    # times_hate = final_df.filter(final_df['prediction'] == 1.0).count()\n",
    "    # print(f'Times hate detected: {times_hate}')\n",
    "    # times_not_hate = final_df.filter(final_df['prediction'] == 0.0).count()\n",
    "    # print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = streaming.writeStream.foreach(live_hate_inference).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Terminated with exception: Writing job aborted',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.utils.StreamingQueryException('Writing job aborted\\n=== Streaming Query ===\\nIdentifier: [id = 50edd787-588c-46e6-9500-7a7a714de56c, runId = 1756f553-dbb2-487c-bc77-afac52b7d38d]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]: {\"logOffset\":0}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nWriteToMicroBatchDataSource ForeachWriterTable(org.apache.spark.sql.execution.python.PythonForeachWriter@41cc0171,Right(org.apache.spark.sql.execution.streaming.sources.ForeachWriterTable$$$Lambda$4160/417186086@15026d32)), 50edd787-588c-46e6-9500-7a7a714de56c, Append\\n+- StreamingExecutionRelation FileStreamSource[file:/C:/data/twitter-stream-parquet-medium], [contributors#18549, coordinates#18550, created_at#18551, display_text_range#18552, entities#18553, extended_entities#18554, extended_tweet#18555, favorite_count#18556L, favorited#18557, filter_level#18558, geo#18559, id#18560L, id_str#18561, in_reply_to_screen_name#18562, in_reply_to_status_id#18563L, in_reply_to_status_id_str#18564, in_reply_to_user_id#18565L, in_reply_to_user_id_str#18566, is_quote_status#18567, lang#18568, place#18569, possibly_sensitive#18570, quote_count#18571L, quoted_status#18572, ... 13 more fields]\\n',\n",
       "                                          'org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:330)\\n\\t at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)',\n",
       "                                          JavaObject id=o3037)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test other approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 2).parquet('C://data/twitter-stream-parquet-medium/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel = trained_pipeline.transform(streaming).select('text','prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = testModel.writeStream.format(\"parquet\").option('path', '../data/parquet_output/').option(\"checkpointLocation\", \"../data/checkpoints/\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '84a5b1f4-2ce0-4666-96d4-9b0344d19233',\n",
       "  'runId': 'c0c536a0-f8a3-48be-9291-f9d39a6920fb',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T18:30:14.163Z',\n",
       "  'batchId': 0,\n",
       "  'numInputRows': 5122,\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'processedRowsPerSecond': 1958.699808795411,\n",
       "  'durationMs': {'addBatch': 1056,\n",
       "   'getBatch': 69,\n",
       "   'latestOffset': 457,\n",
       "   'queryPlanning': 26,\n",
       "   'triggerExecution': 2615,\n",
       "   'walCommit': 577},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': None,\n",
       "    'endOffset': {'logOffset': 0},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 5122,\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'processedRowsPerSecond': 1958.699808795411}],\n",
       "  'sink': {'description': 'FileSink[../data/parquet_output/]',\n",
       "   'numOutputRows': -1}},\n",
       " {'id': '84a5b1f4-2ce0-4666-96d4-9b0344d19233',\n",
       "  'runId': 'c0c536a0-f8a3-48be-9291-f9d39a6920fb',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T18:30:16.779Z',\n",
       "  'batchId': 1,\n",
       "  'numInputRows': 4890,\n",
       "  'inputRowsPerSecond': 1869.2660550458716,\n",
       "  'processedRowsPerSecond': 838.0462724935733,\n",
       "  'durationMs': {'addBatch': 4335,\n",
       "   'getBatch': 104,\n",
       "   'latestOffset': 384,\n",
       "   'queryPlanning': 18,\n",
       "   'triggerExecution': 5835,\n",
       "   'walCommit': 469},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 0},\n",
       "    'endOffset': {'logOffset': 1},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 4890,\n",
       "    'inputRowsPerSecond': 1869.2660550458716,\n",
       "    'processedRowsPerSecond': 838.0462724935733}],\n",
       "  'sink': {'description': 'FileSink[../data/parquet_output/]',\n",
       "   'numOutputRows': -1}},\n",
       " {'id': '84a5b1f4-2ce0-4666-96d4-9b0344d19233',\n",
       "  'runId': 'c0c536a0-f8a3-48be-9291-f9d39a6920fb',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T18:30:22.616Z',\n",
       "  'batchId': 2,\n",
       "  'numInputRows': 5085,\n",
       "  'inputRowsPerSecond': 871.1666952201474,\n",
       "  'processedRowsPerSecond': 1784.2105263157894,\n",
       "  'durationMs': {'addBatch': 1136,\n",
       "   'getBatch': 124,\n",
       "   'latestOffset': 582,\n",
       "   'queryPlanning': 33,\n",
       "   'triggerExecution': 2850,\n",
       "   'walCommit': 507},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 1},\n",
       "    'endOffset': {'logOffset': 2},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 5085,\n",
       "    'inputRowsPerSecond': 871.1666952201474,\n",
       "    'processedRowsPerSecond': 1784.2105263157894}],\n",
       "  'sink': {'description': 'FileSink[../data/parquet_output/]',\n",
       "   'numOutputRows': -1}},\n",
       " {'id': '84a5b1f4-2ce0-4666-96d4-9b0344d19233',\n",
       "  'runId': 'c0c536a0-f8a3-48be-9291-f9d39a6920fb',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T18:30:25.467Z',\n",
       "  'batchId': 3,\n",
       "  'numInputRows': 5210,\n",
       "  'inputRowsPerSecond': 1827.4289722904243,\n",
       "  'processedRowsPerSecond': 1961.5963855421685,\n",
       "  'durationMs': {'addBatch': 1035,\n",
       "   'getBatch': 93,\n",
       "   'latestOffset': 516,\n",
       "   'queryPlanning': 21,\n",
       "   'triggerExecution': 2656,\n",
       "   'walCommit': 548},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 2},\n",
       "    'endOffset': {'logOffset': 3},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 5210,\n",
       "    'inputRowsPerSecond': 1827.4289722904243,\n",
       "    'processedRowsPerSecond': 1961.5963855421685}],\n",
       "  'sink': {'description': 'FileSink[../data/parquet_output/]',\n",
       "   'numOutputRows': -1}},\n",
       " {'id': '84a5b1f4-2ce0-4666-96d4-9b0344d19233',\n",
       "  'runId': 'c0c536a0-f8a3-48be-9291-f9d39a6920fb',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T18:30:28.124Z',\n",
       "  'batchId': 4,\n",
       "  'numInputRows': 4813,\n",
       "  'inputRowsPerSecond': 1811.441475348137,\n",
       "  'processedRowsPerSecond': 1316.8262653898769,\n",
       "  'durationMs': {'addBatch': 1517,\n",
       "   'getBatch': 114,\n",
       "   'latestOffset': 723,\n",
       "   'queryPlanning': 25,\n",
       "   'triggerExecution': 3655,\n",
       "   'walCommit': 796},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 3},\n",
       "    'endOffset': {'logOffset': 4},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 4813,\n",
       "    'inputRowsPerSecond': 1811.441475348137,\n",
       "    'processedRowsPerSecond': 1316.8262653898769}],\n",
       "  'sink': {'description': 'FileSink[../data/parquet_output/]',\n",
       "   'numOutputRows': -1}},\n",
       " {'id': '84a5b1f4-2ce0-4666-96d4-9b0344d19233',\n",
       "  'runId': 'c0c536a0-f8a3-48be-9291-f9d39a6920fb',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T18:30:41.777Z',\n",
       "  'batchId': 5,\n",
       "  'numInputRows': 0,\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'processedRowsPerSecond': 0.0,\n",
       "  'durationMs': {'latestOffset': 21, 'triggerExecution': 21},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 4},\n",
       "    'endOffset': {'logOffset': 4},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 0,\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'processedRowsPerSecond': 0.0}],\n",
       "  'sink': {'description': 'FileSink[../data/parquet_output/]',\n",
       "   'numOutputRows': -1}},\n",
       " {'id': '84a5b1f4-2ce0-4666-96d4-9b0344d19233',\n",
       "  'runId': 'c0c536a0-f8a3-48be-9291-f9d39a6920fb',\n",
       "  'name': None,\n",
       "  'timestamp': '2022-12-20T18:30:51.811Z',\n",
       "  'batchId': 5,\n",
       "  'numInputRows': 0,\n",
       "  'inputRowsPerSecond': 0.0,\n",
       "  'processedRowsPerSecond': 0.0,\n",
       "  'durationMs': {'latestOffset': 14, 'triggerExecution': 14},\n",
       "  'stateOperators': [],\n",
       "  'sources': [{'description': 'FileStreamSource[file:/C:/data/twitter-stream-parquet-medium]',\n",
       "    'startOffset': {'logOffset': 4},\n",
       "    'endOffset': {'logOffset': 4},\n",
       "    'latestOffset': None,\n",
       "    'numInputRows': 0,\n",
       "    'inputRowsPerSecond': 0.0,\n",
       "    'processedRowsPerSecond': 0.0}],\n",
       "  'sink': {'description': 'FileSink[../data/parquet_output/]',\n",
       "   'numOutputRows': -1}}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate Source Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame\n",
    "df = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
