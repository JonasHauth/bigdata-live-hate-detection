{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architektur\n",
    "Architektur Training:\n",
    "- Data Lake (Annotierte Trainingsdaten ca. 1.2 Mio Tweets)\n",
    "- Trainingspipeline (Spark ML) > Modell für Inferenz\n",
    "\n",
    "Architektur Live-Hate-Classification:\n",
    "- Data Source (Live simuliert mit Twitter Grabs ca. 20 GB lokal)\n",
    "- Inferenzpipeline (Spark Streaming)\n",
    "- Data Lake (Gefunde Hate Speech wird gespeichert)\n",
    "\n",
    "Architektur Hate-Report\n",
    "- Data Source (Gefunde Hate Speech)\n",
    "- Analysen (Wörter, Nutzer, Ländern, ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version:  3.3.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Jonas-Surface:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>multi_class_text_classifiter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2bc45a29f40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, lower\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "MAX_MEMORY = \"6g\"\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('multi_class_text_classifiter')\\\n",
    "                    .master(\"local[4]\") \\\n",
    "                    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "                    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.1\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten aus dem Data Lake (Parquet File) laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198584"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = spark.read.parquet('../data/parquet_data')\n",
    "spark_df = spark_df.withColumnRenamed(\"tweet_text\",\"text\")\n",
    "spark_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Methode für die Bereinigung der Texte (z.B. Groß- und Kleinschreibung, Sondernzeichen, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+\n",
      "|              index|                text|majority_label|\n",
      "+-------------------+--------------------+--------------+\n",
      "|1108866829991272448|@ finna fuck pont...|             0|\n",
      "|1058874314303320064|t don mind me ’ i...|             1|\n",
      "|1109486326477438976|a law played jude...|             0|\n",
      "|1062399239337140224|review of heart b...|             0|\n",
      "|1113926202006360064|nigga when the yo...|             0|\n",
      "+-------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\") # Remove links\n",
    "  c = regexp_replace(c, \"(\\\\n)|\\n|\\r|\\t\", \"\") # Remove CR, tab, and LR\n",
    "  c = regexp_replace(c, \"(?:(?:[0-9]{2}[:\\/,]){2}[0-9]{2,4})\", \"\") # Remove dates\n",
    "  c = regexp_replace(c, \"@([A-Za-z0-9_]+)\", \"\") # Remove usernames\n",
    "  c = regexp_replace(c, \"[0-9]\", \"\") # Remove numbers\n",
    "  c = regexp_replace(c, \"\\:|\\/|\\#|\\.|\\?|\\!|\\&|\\\"|\\,\", \"\") # Remove symbols\n",
    "  return c\n",
    "\n",
    "spark_df = spark_df.withColumn(\"text\", clean_text(col(\"text\")))\n",
    "\n",
    "spark_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den gesamten Datensatz oder einen Anteil des Datensatzes für das Training auswählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_sample = spark_df#.sample() #fraction=0.1\n",
    "spark_df_sample.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Für das Features Engineering wurden zwei Methoden für die Umwandlung der natürlichsprachlichen Texte in strukturierte Daten untersucht.\n",
    "- Tokenization und Word2Vec (Embedding): Basierend auf den einzelnen Wörtern in einem Tweet, wird für jeden Satz ein Vektor mit 300 Dimensionen gebildet. \n",
    "- Tokenization und CountVectorizer: Nach der Aufteilung der Tweets in einzelne Wörter, werden die 10000 häufigsten Wörter in jedem Tweet gezählt. Es entsteht ein Vektor mit 10000 Dimensionen.\n",
    "\n",
    "Die beste Leistung konnte mit dem CountVectorizer erzielt werden. \n",
    "Weitere Optionen, wie z.B. TF-IDF oder Embeddings von großen Sprachmodellen wurden nicht untersucht. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization und Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Word2Vec\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import Tokenizer\n",
    "# from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# # 'We hate religion' > 'We' 'hate' 'religion'\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "\n",
    "# # 'We' > (0.000, 0.032432, ...) 300 Dimensionen\n",
    "# w2v = Word2Vec(vectorSize=300, minCount=0, inputCol=\"tokens\", outputCol=\"features\")\n",
    "\n",
    "# doc2vec_pipeline = Pipeline(stages=[tokenizer, w2v])\n",
    "# doc2vec_model = doc2vec_pipeline.fit(spark_df_sample)\n",
    "# doc2vecs_df = doc2vec_model.transform(spark_df_sample)\n",
    "\n",
    "# doc2vec_model.write().overwrite().save(\"../models/prep_tok2vec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization und CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "\n",
    "# 'We hate religion' > 'We' 'hate' 'religion'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Remove stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Term frequency\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "doc2tf_pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "doc2tf_model = doc2tf_pipeline.fit(spark_df_sample)\n",
    "doc2tf_df = doc2tf_model.transform(spark_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2tf_model.write().overwrite().save(\"../models/prep_tok2tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Für das Training wurden die klassischen ML-Modelle Logistic Regression und Naive Bayes untersucht. Für das weitere Vorgehen wurde das Naive Bayes-Modell ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Tok2Tf\n",
    "hate_train_df, hate_test_df = doc2tf_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 958538\n",
      "Times hate in training: 236156\n",
      "Times not hate in training: 722382\n",
      "Test Dataset Count: 240046\n",
      "Times hate in test: 59612\n",
      "Times not hate in test: 180434\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Count: \" + str(hate_train_df.count()))\n",
    "times_hate = hate_train_df.filter(hate_train_df['majority_label'] > 0.0).count()\n",
    "print(f'Times hate in training: {times_hate}')\n",
    "times_not_hate = hate_train_df.filter(hate_train_df['majority_label'] == 0.0).count()\n",
    "print(f'Times not hate in training: {times_not_hate}')\n",
    "\n",
    "print(\"Test Dataset Count: \" + str(hate_test_df.count()))\n",
    "times_hate = hate_test_df.filter(hate_test_df['majority_label'] > 0.0).count()\n",
    "print(f'Times hate in test: {times_hate}')\n",
    "times_not_hate = hate_test_df.filter(hate_test_df['majority_label'] == 0.0).count()\n",
    "print(f'Times not hate in test: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# lr_classifier = LogisticRegression(family=\"multinomial\", labelCol=\"majority_label\", featuresCol=\"features\")\n",
    "\n",
    "# lr_classifier_pipeline = Pipeline(stages=[lr_classifier])\n",
    "# lr_trained_pipeline = lr_classifier_pipeline.fit(hate_train_df)\n",
    "# predictions = lr_trained_pipeline.transform(hate_test_df)\n",
    "\n",
    "# lr_model_evaluator = MulticlassClassificationEvaluator(\n",
    "#     labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# accuracy = lr_model_evaluator.evaluate(predictions)\n",
    "# print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "# times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "# print(f'Times hate detected: {times_hate}')\n",
    "# times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "# print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_trained_pipeline.write().overwrite().save(\"../models/model_lr\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "classifier = NaiveBayes(smoothing=1, labelCol=\"majority_label\", featuresCol=\"features\")\n",
    "\n",
    "classifier_pipeline = Pipeline(stages=[classifier])\n",
    "predictions = classifier_pipeline.fit(hate_train_df).transform(hate_test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.765616\n",
      "Times hate detected: 50313\n",
      "Times not hate detected: 189733\n"
     ]
    }
   ],
   "source": [
    "model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = model_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "print(f'Times hate detected: {times_hate}')\n",
    "times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pipeline.write().overwrite().save(\"../models/model_nb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den gesamten Datensatz oder einen Anteil des Datensatzes für das Training auswählen. Je größer der Trainingsdatensatz, desto länger benötigt die MLlib Pipeline für die Ausführung.\n",
    "\n",
    "Benchmark der Trainingszeiten MLlib:\n",
    "| Anzahl Trainingsdatensätze | Mean | Standard Deviation |\n",
    "| --- | --- | --- |\n",
    "| 120461 | 3.30 | 0.42 |\n",
    "|599259 | 8.98 | 0.63 |\n",
    "| 1198584 | 17.4 | 1.03 |\n",
    "\n",
    "Benchmark der Trainingszeiten Sklearn:\n",
    "| Anzahl Trainingsdatensätze | Mean | Standard Deviation |\n",
    "| --- | --- | --- |\n",
    "| 120461 | 2.96 | 0.31 |\n",
    "| 599259 | 11.88 | 0.44 |\n",
    "| 1198584 | 22.56 | 0.43 |\n",
    "\n",
    "### Durchführung Benchmarks: \n",
    "Local Spark Environment (3.3.1)\n",
    "- 4 Cores, 8 Threats (1.8 Ghz Base Clock)\n",
    "- 8 GB RAM (shared with OS)\n",
    "\n",
    "Ergebnisse:\n",
    "- In unserem Test liefert Spark MLLib bei kleinen Datenmengen minimal schlechtere Ergebnisse. Dies könnte z.B. durch Computational Overhead für die Spark Infrastruktur sein. Allerdings ist die Abweichung von durchschnittlich 0,3s sehr gering.\n",
    "- In unserem Test skaliert die Spark MLLib Trainingspipeline bei größer werdenen Datenmengen besser. Dies könnte durch die verteilte Berechnung in der Spark Infrastruktur erreicht werden.  \n",
    "\n",
    "Einschränkungen:\n",
    "- Trotz größter Sorgfalt können beeinflussende Faktoren bei der manuellen Durchführung der Tests (z.B. durch Hintergrundprozsse) nicht ausgeschlossen werden. \n",
    "- Zudem sind die Ergebnisse stark abhängig von der jeweiligen Implementierung mit dem Framework. \n",
    "- Für das schnelle Prototyping wurde eine lokale Spark-Installation gewählt. Diese kann maximal über die Ressourcen des Host-PCs verfügen. Vorteile durch die vertikale Skalierung können somit nicht mit ausgewertet werden.\n",
    "- Die festgestellten Abweichungen lassen somit keine signifikanten, gesicherten Aussagen über den Vergleich der Leistun der beiden Bibliotheken zu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119883"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark_df_sample = spark_df.sample(fraction=0.1) \n",
    "spark_df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_train_df, hate_test_df = spark_df_sample.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#120461: 3.7s, 3.7s, 3.1s, 2.7s, 3.3s\n",
    "#599259: 9.2s, 8.9s, 8.7s, 8.2s, 9.9s\n",
    "#1198584: 17.6s, 17.5s, 15.9s, 17.2s, 18.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'We hate religion' > 'We' 'hate' 'religion'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Remove stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Term frequency\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "# Classifier\n",
    "classifier = NaiveBayes(smoothing=1, labelCol=\"majority_label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, classifier])\n",
    "trained_inference_pipeline = inference_pipeline.fit(hate_train_df)\n",
    "predictions = trained_inference_pipeline.transform(hate_test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save MLlib Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_inference_pipeline.write().overwrite().save(\"../models/mllib_model_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.759202\n"
     ]
    }
   ],
   "source": [
    "model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = model_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "# times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "# print(f'Times hate detected: {times_hate}')\n",
    "# times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "# print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1198584 entries, 0 to 1198583\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   index           1198584 non-null  int64 \n",
      " 1   text            1198584 non-null  object\n",
      " 2   majority_label  1198584 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 27.4+ MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pandas_df = spark_df.toPandas()\n",
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 599259 entries, 73519 to 1160324\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   index           599259 non-null  int64 \n",
      " 1   text            599259 non-null  object\n",
      " 2   majority_label  599259 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "pandas_df_sample = pandas_df.sample(n=599259)\n",
    "pandas_df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pandas_df_sample['text'], pandas_df_sample['majority_label'], train_size = 0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#120461: 2.9s, 2.8s, 2.9s, 3.5s, 2.7s\n",
    "#599259: 11.4s, 11.8s, 11.7s, 11.9s, 12.6s\n",
    "#1198584: 22.1s, 22.4s, 22.8s, 22.3s, 23.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = CountVectorizer(stop_words='english', min_df=2, max_features=10000)\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline([\n",
    "    ('vect', countVec),\n",
    "    ('clf', clf),\n",
    "])\n",
    "\n",
    "trained_inference_pipeline = inference_pipeline.fit(X_train, y_train)\n",
    "y_pred = trained_inference_pipeline.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/sklearn_model_nb.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(trained_inference_pipeline, '../models/sklearn_model_nb.pkl')\n",
    "# pipeline = joblib.load('pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7683893468611287"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small MLLib mean: 3.3000000000000003 stdev: 0.4242640687119285\n",
      "Medium MLLib mean: 8.98 stdev: 0.6300793600809349\n",
      "Complete MLLib mean: 17.4 stdev: 1.0368220676663862\n",
      "Small Sklearn mean: 2.96 stdev: 0.31304951684997057\n",
      "Medium Sklearn mean: 11.88 stdev: 0.44384682042344276\n",
      "Complete Sklearn mean: 22.56 stdev: 0.4393176527297754\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "small_mllib = [3.7, 3.7, 3.1, 2.7, 3.3]\n",
    "print(f\"Small MLLib mean: {mean(small_mllib)} stdev: {stdev(small_mllib)}\")\n",
    "medium_mllib = [9.2, 8.9, 8.7, 8.2, 9.9]\n",
    "print(f\"Medium MLLib mean: {mean(medium_mllib)} stdev: {stdev(medium_mllib)}\")\n",
    "complete_mllib = [17.6, 17.5, 15.9, 17.2, 18.8]\n",
    "print(f\"Complete MLLib mean: {mean(complete_mllib)} stdev: {stdev(complete_mllib)}\")\n",
    "small_sklearn = [2.9, 2.8, 2.9, 3.5, 2.7]\n",
    "print(f\"Small Sklearn mean: {mean(small_sklearn)} stdev: {stdev(small_sklearn)}\")\n",
    "medium_sklearn = [11.4, 11.8, 11.7, 11.9, 12.6]\n",
    "print(f\"Medium Sklearn mean: {mean(medium_sklearn)} stdev: {stdev(medium_sklearn)}\")\n",
    "complete_sklearn = [22.1, 22.4, 22.8, 22.3, 23.2]\n",
    "print(f\"Complete Sklearn mean: {mean(complete_sklearn)} stdev: {stdev(complete_sklearn)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "28d6d9e31a694f2aba28d42b1019d9365f56410d563150feaee59905aa4508a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
