{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Data Charakterstics\n",
    "1. Volume: Tweets sind in der Regel kurze Nachrichten mit begrenztem Umfang, aber die Anzahl der Tweets, die jeden Tag gepostet werden, ist enorm. Laut Twitter selbst generieren Nutzer mehr als 500 Millionen Tweets pro Tag. Diese große Menge an Daten erfordert eine effektive Verarbeitungstechnologie, um die Daten zu speichern, zu verarbeiten und zu analysieren.\n",
    "\n",
    "Text ohne Metadaten (-> reiner Text):\n",
    "- 1 Tweet hat also im Schnitt: 28 * 8 Bits = 224 Bits = 28 Byte\n",
    "- hochgerechnet auf die Zahlen von oben bedeutet das also:\n",
    "- 6.000 Tweets pro Sekunde      --> 168.000 Byte = 168 Kilobyte pro Tag\n",
    "- = 360.000 Tweets pro Minute    --> 10.080.000 Byte = 10,08 Megabyte pro Tag\n",
    "- = 21.600.000 Tweets pro Stunde     --> 604.800.000 Byte = 604,8 Megabyte pro Tag\n",
    "- = 518.400.000 Tweets pro Tag    --> 14.515.200.000 Byte = 14,51 Gigabyte pro Tag\n",
    "\n",
    "\n",
    "\n",
    "Text mit Metadaten und ohne Bilder/Videos:\n",
    "- 2656 Tweets --> 14.673.664 Bytes --> 5.524 Byte pro Tweet\n",
    "- 1 Tweet hat also im Schnitt inkl. Metadaten: 5.524 Byte\n",
    "- hochgerechnet auf die Zahlen von oben bedeutet das also:\n",
    "- 6.000 Tweets pro Sekunde --> 33.144.000 Byte = 33,144 Megabyte pro Tag\n",
    "- = 360.000 Tweets pro Minute    --> 1.988.640.000 Byte = 1,988 Gigabyte pro Tag\n",
    "- = 21.600.000 Tweets pro Stunde     --> 119.318.400.000 Byte = 119,3 Gigabyte pro Tag\n",
    "- = 518.400.000 Tweets pro Tag    --> 2.863.641.600.000 Byte = 2,86 Terabyte pro Tag\n",
    "\n",
    "2. Variety: Tweets enthalten verschiedene Arten von Daten wie Text, Bilder, Videos, Links und Metadaten. Diese Vielfalt an Daten erfordert eine Technologie, die in der Lage ist, verschiedene Datentypen zu verarbeiten und zu analysieren.\n",
    "\n",
    "3. Velocity: Tweets werden in Echtzeit gepostet, was bedeutet, dass die Datenströme schnell und kontinuierlich sind.  \n",
    "Maximale Anzahl an Tweets die jemals gemessen wurde: 618 725 Tweets/min\n",
    "\n",
    "Warum kann das Problem nicht mit herkömmlichen Storage/Analyse/Datenbank-technologien gelöst werden?\n",
    "- Hate Speech-Analyse ist mit herkömmlichen mitteln möglich, jedoch nicht in dem Volumen und Geschwindigkeit.\n",
    "- Der Trainingsdatensatz sollte ständig erweitert und neue Modelle trainiert werden -> stetig komplexere Analysen und Trainingsdurchläufe.\n",
    "\n",
    "Setup/Bootstrapping Kosten\n",
    "- Hardware-Kosten: Speicher, CPU, Rack ...\n",
    "- Potenzielle Integrationskosten: Anbindung an vorhandenen Systeme von Twitter\n",
    "\n",
    "Skew und Bias der Daten:\n",
    "- Daten sind in in Realität deutlich Biased (eine viel größere Anzahl an Non-Hate-Speech liegt vor). In unserem Trainingsdatensatz sind die Labels etwa gleichverteilt. Die Tweets können somit ohne großen skew zwischen den Knoten verteilt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version:  3.3.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>multi_class_text_classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e8815cc6a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, lower\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "MAX_MEMORY = \"6g\"\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('multi_class_text_classifier')\\\n",
    "                    .master(\"local[8]\") \\\n",
    "                    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "                    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingsdaten aus dem Data Lake (Parquet File) laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198584"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = spark.read.parquet('../data/parquet_data')\n",
    "spark_df = spark_df.withColumnRenamed(\"tweet_text\",\"text\")\n",
    "spark_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Methode für die Bereinigung der Texte (z.B. Groß- und Kleinschreibung, Sondernzeichen, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+\n",
      "|              index|                text|majority_label|\n",
      "+-------------------+--------------------+--------------+\n",
      "|1108866829991272448|@ finna fuck pont...|             0|\n",
      "|1058874314303320064|t don mind me ’ i...|             1|\n",
      "|1109486326477438976|a law played jude...|             0|\n",
      "|1062399239337140224|review of heart b...|             0|\n",
      "|1113926202006360064|nigga when the yo...|             0|\n",
      "+-------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\") # Remove links\n",
    "  c = regexp_replace(c, \"(\\\\n)|\\n|\\r|\\t\", \"\") # Remove CR, tab, and LR\n",
    "  c = regexp_replace(c, \"(?:(?:[0-9]{2}[:\\/,]){2}[0-9]{2,4})\", \"\") # Remove dates\n",
    "  c = regexp_replace(c, \"@([A-Za-z0-9_]+)\", \"\") # Remove usernames\n",
    "  c = regexp_replace(c, \"[0-9]\", \"\") # Remove numbers\n",
    "  c = regexp_replace(c, \"\\:|\\/|\\#|\\.|\\?|\\!|\\&|\\\"|\\,\", \"\") # Remove symbols\n",
    "  return c\n",
    "\n",
    "spark_df = spark_df.withColumn(\"text\", clean_text(col(\"text\")))\n",
    "\n",
    "spark_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den gesamten Datensatz oder einen Anteil des Datensatzes für das Training auswählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198584"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_sample = spark_df#.sample() #fraction=0.1\n",
    "spark_df_sample.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Für das Features Engineering wurden zwei Methoden für die Umwandlung der natürlichsprachlichen Texte in strukturierte Daten untersucht.\n",
    "- Tokenization und Word2Vec (Embedding): Basierend auf den einzelnen Wörtern in einem Tweet, wird für jeden Satz ein Vektor mit 300 Dimensionen gebildet. \n",
    "- Tokenization und CountVectorizer: Nach der Aufteilung der Tweets in einzelne Wörter, werden die 10000 häufigsten Wörter in jedem Tweet gezählt. Es entsteht ein Vektor mit 10000 Dimensionen.\n",
    "\n",
    "Die beste Leistung konnte mit dem CountVectorizer erzielt werden. \n",
    "Weitere Optionen, wie z.B. TF-IDF oder Embeddings von großen Sprachmodellen wurden nicht untersucht. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization und Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Word2Vec\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import Tokenizer\n",
    "# from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# # 'We hate religion' > 'We' 'hate' 'religion'\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "\n",
    "# # 'We' > (0.000, 0.032432, ...) 300 Dimensionen\n",
    "# w2v = Word2Vec(vectorSize=300, minCount=0, inputCol=\"tokens\", outputCol=\"features\")\n",
    "\n",
    "# doc2vec_pipeline = Pipeline(stages=[tokenizer, w2v])\n",
    "# doc2vec_model = doc2vec_pipeline.fit(spark_df_sample)\n",
    "# doc2vecs_df = doc2vec_model.transform(spark_df_sample)\n",
    "\n",
    "# doc2vec_model.write().overwrite().save(\"../models/prep_tok2vec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization und CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "\n",
    "# 'We hate religion' > 'We' 'hate' 'religion'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Remove stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Term frequency\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "doc2tf_pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "doc2tf_model = doc2tf_pipeline.fit(spark_df_sample)\n",
    "doc2tf_df = doc2tf_model.transform(spark_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2tf_model.write().overwrite().save(\"../models/prep_tok2tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Für das Training wurden die klassischen ML-Modelle Logistic Regression und Naive Bayes untersucht. Für das weitere Vorgehen wurde das Naive Bayes-Modell ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Tok2Tf\n",
    "hate_train_df, hate_test_df = doc2tf_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 958700\n",
      "Times hate in training: 236164\n",
      "Times not hate in training: 722536\n",
      "Test Dataset Count: 239884\n",
      "Times hate in test: 59604\n",
      "Times not hate in test: 180280\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Count: \" + str(hate_train_df.count()))\n",
    "times_hate = hate_train_df.filter(hate_train_df['majority_label'] > 0.0).count()\n",
    "print(f'Times hate in training: {times_hate}')\n",
    "times_not_hate = hate_train_df.filter(hate_train_df['majority_label'] == 0.0).count()\n",
    "print(f'Times not hate in training: {times_not_hate}')\n",
    "\n",
    "print(\"Test Dataset Count: \" + str(hate_test_df.count()))\n",
    "times_hate = hate_test_df.filter(hate_test_df['majority_label'] > 0.0).count()\n",
    "print(f'Times hate in test: {times_hate}')\n",
    "times_not_hate = hate_test_df.filter(hate_test_df['majority_label'] == 0.0).count()\n",
    "print(f'Times not hate in test: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# lr_classifier = LogisticRegression(family=\"multinomial\", labelCol=\"majority_label\", featuresCol=\"features\")\n",
    "\n",
    "# lr_classifier_pipeline = Pipeline(stages=[lr_classifier])\n",
    "# lr_trained_pipeline = lr_classifier_pipeline.fit(hate_train_df)\n",
    "# predictions = lr_trained_pipeline.transform(hate_test_df)\n",
    "\n",
    "# lr_model_evaluator = MulticlassClassificationEvaluator(\n",
    "#     labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# accuracy = lr_model_evaluator.evaluate(predictions)\n",
    "# print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "# times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "# print(f'Times hate detected: {times_hate}')\n",
    "# times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "# print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_trained_pipeline.write().overwrite().save(\"../models/model_lr\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "classifier = NaiveBayes(smoothing=1, labelCol=\"majority_label\", featuresCol=\"features\")\n",
    "\n",
    "classifier_pipeline = Pipeline(stages=[classifier])\n",
    "predictions = classifier_pipeline.fit(hate_train_df).transform(hate_test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.766279\n",
      "Times hate detected: 50350\n",
      "Times not hate detected: 189534\n"
     ]
    }
   ],
   "source": [
    "model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = model_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "print(f'Times hate detected: {times_hate}')\n",
    "times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pipeline.write().overwrite().save(\"../models/model_nb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den gesamten Datensatz oder einen Anteil des Datensatzes für das Training auswählen. Je größer der Trainingsdatensatz, desto länger benötigten die Pipeline für die Ausführung. Für die  Ausführung ausgewählt wurden drei Datensätze kuriert. Weitere Benchmarks zu den Datensätzen sind in dem Notebook compare_data_storage_formats zu finden.\n",
    "\n",
    "In dem nachstehenden Notebook ist die Implementierung in MLLib und eine Vergleichsimplementierung in Sklearn abgebildet. Den Source-Code für die Ausführung des Benchmark mit varierenden Kernanzahlen für die MLLib-Implementierung findet sich in der Datei: \"Benchmark_Training_Pipeline_Spark.py\"\n",
    "Die erhobenen Messergebnisse finden sich in dem Ordner Benchmark-Ergebnisse.\n",
    "\n",
    "Für jede Konfiguration wurden 5 Testabläufe durchgeführt und der Mittelwert sowie die Standardabweichung berechnet.\n",
    "\n",
    "Benchmark der Trainingszeiten MLlib:\n",
    "| Anzahl Trainingsdatensätze | Mean 1 Kern | Mean 2 Kerne | Mean 4 Kerne | Mean 8 Kerne |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| small | 5.04s (0.21s) | 3.32s (0.19s) | 2.47s (0.31s) | 2.42s (0.23s) |\n",
    "| medium | 17.35s (0.44s) | 10.25s (0.08s) | 6.44s (0.10s) | 5.86s (0.17s) |\n",
    "| complete | 31.85s (0.75s) | 18.88s (0.36s) | 12.47s (0.33s) | 11.25s (0.45s) |\n",
    "\n",
    "Benchmark der Trainingszeiten Sklearn:\n",
    "| Anzahl Trainingsdatensätze | Mean |\n",
    "| --- | --- |\n",
    "| 120461 | 2.96s (0.31s) |\n",
    "| 599259 | 11.88s (0.44s) |\n",
    "| 1198584 | 22.56s (0.43s) |\n",
    "\n",
    "Ergebnisse:\n",
    "- In unserem Test liefert Spark MLLib bei kleinen Datenmengen und wenigen Kernen minimal schlechtere Ergebnisse. Dies könnte z.B. durch Computational Overhead für die Spark Infrastruktur sein. Allerdings ist die Abweichung sehr gering.\n",
    "- In unserem Test skaliert die Spark MLLib Trainingspipeline sehr gut mit steigender Anzahl der lokal zur Verfügung gestellten Kerne. Diese deutet auf eine gute horizontale Skalierbarkeit hin.\n",
    "- In unserem Test skaliert die Spark MLLib Trainingspipeline bei größer werdenen Datenmengen besser. Dies könnte durch die verteilte Berechnung in der Spark Infrastruktur erreicht werden.  \n",
    "\n",
    "### Anmerkungen zur Durchführung des Benchmarks: \n",
    "Local Spark Environment (3.3.1)\n",
    "- 4 Cores, 8 Threats (1.8 Ghz Base Clock)\n",
    "- 8 GB RAM (shared with OS)\n",
    "\n",
    "Einschränkungen:\n",
    "- Trotz größter Sorgfalt können beeinflussende Faktoren bei der manuellen Durchführung der Tests (z.B. durch Hintergrundprozsse) nicht ausgeschlossen werden. \n",
    "- Die Ergebnisse des Benchmark sind stark abhängig von der Implementierung in den Frameworks.  \n",
    "- Für das schnelle Prototyping wurde eine lokale Spark-Installation gewählt. Diese kann maximal über die Ressourcen des Host-PCs verfügen. \n",
    "- Die Leistungssteigerungen von 8 auf 4 Kerne fallen erwartungsgemäß nicht sehr groß aus, da hier nicht mehr physische Kerne als vorher genutzt werden können."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598566"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark_df_sample = spark_df.sample(fraction=0.1) \n",
    "spark_df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_train_df, hate_test_df = spark_df_sample.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'We hate religion' > 'We' 'hate' 'religion'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Remove stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Term frequency\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "# Classifier\n",
    "classifier = NaiveBayes(smoothing=1, labelCol=\"majority_label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, classifier])\n",
    "trained_inference_pipeline = inference_pipeline.fit(hate_train_df)\n",
    "predictions = trained_inference_pipeline.transform(hate_test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save MLlib Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_inference_pipeline.write().overwrite().save(\"../models/mllib_model_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.767052\n"
     ]
    }
   ],
   "source": [
    "model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = model_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1198584 entries, 0 to 1198583\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   index           1198584 non-null  int64 \n",
      " 1   text            1198584 non-null  object\n",
      " 2   majority_label  1198584 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 27.4+ MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pandas_df = spark_df.toPandas()\n",
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 599259 entries, 1106893 to 314064\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   index           599259 non-null  int64 \n",
      " 1   text            599259 non-null  object\n",
      " 2   majority_label  599259 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "pandas_df_sample = pandas_df.sample(n=599259)\n",
    "pandas_df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pandas_df_sample['text'], pandas_df_sample['majority_label'], train_size = 0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#120461: 2.9s, 2.8s, 2.9s, 3.5s, 2.7s\n",
    "#599259: 11.4s, 11.8s, 11.7s, 11.9s, 12.6s\n",
    "#1198584: 22.1s, 22.4s, 22.8s, 22.3s, 23.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = CountVectorizer(stop_words='english', min_df=2, max_features=10000)\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline([\n",
    "    ('vect', countVec),\n",
    "    ('clf', clf),\n",
    "])\n",
    "\n",
    "trained_inference_pipeline = inference_pipeline.fit(X_train, y_train)\n",
    "y_pred = trained_inference_pipeline.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/sklearn_model_nb.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(trained_inference_pipeline, '../models/sklearn_model_nb.pkl')\n",
    "# pipeline = joblib.load('pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7686563428228148"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small MLLib mean: 3.3000000000000003 stdev: 0.4242640687119285\n",
      "Medium MLLib mean: 8.98 stdev: 0.6300793600809349\n",
      "Complete MLLib mean: 17.4 stdev: 1.0368220676663862\n",
      "Small Sklearn mean: 2.96 stdev: 0.31304951684997057\n",
      "Medium Sklearn mean: 11.88 stdev: 0.44384682042344276\n",
      "Complete Sklearn mean: 22.56 stdev: 0.4393176527297754\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "small_sklearn = [2.9, 2.8, 2.9, 3.5, 2.7]\n",
    "print(f\"Small Sklearn mean: {mean(small_sklearn)} stdev: {stdev(small_sklearn)}\")\n",
    "medium_sklearn = [11.4, 11.8, 11.7, 11.9, 12.6]\n",
    "print(f\"Medium Sklearn mean: {mean(medium_sklearn)} stdev: {stdev(medium_sklearn)}\")\n",
    "complete_sklearn = [22.1, 22.4, 22.8, 22.3, 23.2]\n",
    "print(f\"Complete Sklearn mean: {mean(complete_sklearn)} stdev: {stdev(complete_sklearn)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fb95653776c9bc119c508621720686c5ecc2cd3dc0e93618745aec1af9c6006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
