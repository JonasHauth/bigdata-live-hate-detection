{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granulare Benchmarks:\n",
    "- Speichergröße (Json, Excel, vs. Parquet)\n",
    "- Inferenz Dauer (Sklearn, Spark ML)\n",
    "- Troughput (Spark Streaming, Wie viele Tweets pro Sekunde verarbeiten)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architektur\n",
    "Architektur Training:\n",
    "- Data Lake (Annotierte Trainingsdaten ca. 1.2 Mio Tweets)\n",
    "- Trainingspipeline (Spark ML) > Modell für Inferenz\n",
    "\n",
    "Architektur Live-Hate-Classification:\n",
    "- Data Source (Live simuliert mit Twitter Grabs ca. 20 GB lokal)\n",
    "- Inferenzpipeline (Spark Streaming)\n",
    "- Data Lake (Gefunde Hate Speech wird gespeichert)\n",
    "\n",
    "Architektur Hate-Report\n",
    "- Data Source (Gefunde Hate Speech)\n",
    "- Analysen (Wörter, Nutzer, Ländern, ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version:  3.3.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Jonas-Surface:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>multi_class_text_classifiter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29685a8c6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, lower\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "MAX_MEMORY = \"6g\"\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('multi_class_text_classifiter')\\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "                    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.1\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198584"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = spark.read.parquet('../data/parquet_data')\n",
    "spark_df = spark_df.withColumnRenamed(\"tweet_text\",\"text\")\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+\n",
      "|              index|          tweet_text|majority_label|\n",
      "+-------------------+--------------------+--------------+\n",
      "|1108866829991272448|@ finna fuck Pont...|             0|\n",
      "|1058874314303320064|t don mind me, ’ ...|             1|\n",
      "|1109486326477438976|A Law played jude...|             0|\n",
      "|1062399239337140224|Review of: Heart ...|             0|\n",
      "|1113926202006360064|Nigga when the yo...|             0|\n",
      "+-------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark_df = spark_df.withColumn(\"text\",regexp_replace(col('text'), '\\d+', ''))\n",
    "# spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+\n",
      "|              index|                text|majority_label|\n",
      "+-------------------+--------------------+--------------+\n",
      "|1108866829991272448|@ finna fuck pont...|             0|\n",
      "|1058874314303320064|t don mind me ’ i...|             1|\n",
      "|1109486326477438976|a law played jude...|             0|\n",
      "|1062399239337140224|review of heart b...|             0|\n",
      "|1113926202006360064|nigga when the yo...|             0|\n",
      "+-------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\") # Remove links\n",
    "  c = regexp_replace(c, \"(\\\\n)|\\n|\\r|\\t\", \"\") # Remove CR, tab, and LR\n",
    "  c = regexp_replace(c, \"(?:(?:[0-9]{2}[:\\/,]){2}[0-9]{2,4})\", \"\") # Remove dates\n",
    "  c = regexp_replace(c, \"@([A-Za-z0-9_]+)\", \"\") # Remove usernames\n",
    "  c = regexp_replace(c, \"[0-9]\", \"\") # Remove numbers\n",
    "  c = regexp_replace(c, \"\\:|\\/|\\#|\\.|\\?|\\!|\\&|\\\"|\\,\", \"\") # Remove symbols\n",
    "  return c\n",
    "\n",
    "spark_df = spark_df.withColumn(\"text\", clean_text(col(\"text\")))\n",
    "\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198584"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark_df_sample =  spark_df\n",
    "spark_df_sample = spark_df#.sample() #fraction=0.1\n",
    "spark_df_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization und Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Word2Vec\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import Tokenizer\n",
    "# from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# # 'We hate religion' > 'We' 'hate' 'religion'\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "\n",
    "# # 'We' > (0.000, 0.032432, ...) 300 Dimensionen\n",
    "# w2v = Word2Vec(vectorSize=300, minCount=0, inputCol=\"tokens\", outputCol=\"features\")\n",
    "\n",
    "# doc2vec_pipeline = Pipeline(stages=[tokenizer, w2v])\n",
    "# doc2vec_model = doc2vec_pipeline.fit(spark_df_sample)\n",
    "# doc2vecs_df = doc2vec_model.transform(spark_df_sample)\n",
    "\n",
    "# doc2vec_model.write().overwrite().save(\"../models/prep_tok2vec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization und TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "\n",
    "# 'We hate religion' > 'We' 'hate' 'religion'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Remove stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Term frequency\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "doc2tf_pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "doc2tf_model = doc2tf_pipeline.fit(spark_df_sample)\n",
    "doc2tf_df = doc2tf_model.transform(spark_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2tf_model.write().overwrite().save(\"../models/prep_tok2tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Tok2Vec\n",
    "# hate_train_df, hate_test_df = doc2vecs_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Data from Tok2Tf\n",
    "hate_train_df, hate_test_df = doc2tf_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 958927\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|              index|                text|majority_label|              tokens|            filtered|            features|\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|1023940590382268416|Eamon0303 @ @ CNN...|             0|[eamon0303, cnn, ...|[eamon0303, cnn, ...|(10000,[0,1,17,12...|\n",
      "|1023940826882293760|@ meloIigya re yo...|             1|[meloiigya, re, y...|[meloiigya, re, b...|(10000,[0,1,10,43...|\n",
      "|1023940897346658304|Some of y ’ just ...|             0|[some, of, y, jus...|[y, darts, throwi...|(10000,[0,1,42,47...|\n",
      "|1023942214844657664|Ye ‘ either sound...|             0|[ye, either, soun...|[ye, either, soun...|(10000,[0,1,3,30,...|\n",
      "|1023942220838264832|@ wannabwinehouse...|             0|[wannabwinehouse,...|[wannabwinehouse,...|(10000,[0,1,3,415...|\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Times hate in training: 236666\n",
      "Times not hate in training: 722261\n",
      "Test Dataset Count: 239657\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|              index|                text|majority_label|              tokens|            filtered|            features|\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|1023943945670148096|Thuggin 24 / anyo...|             0|[thuggin, 24, any...|[thuggin, 24, any...|(10000,[0,1,3,43,...|\n",
      "|1023945825528020992|The I behavior af...|             0|[the, i, behavior...|[behavior, seeing...|(10000,[0,1,4,7,5...|\n",
      "|1023946157582700544|@ racist tedcruz ...|             1|[racist, tedcruz,...|[racist, tedcruz,...|(10000,[0,1,21,28...|\n",
      "|1023946868407951360|Niggy BanditSnek ...|             0|[niggy, banditsne...|[niggy, banditsne...|(10000,[0,1,3,110...|\n",
      "|1023954804798717952|Nigger @ https Ma...|             1|[nigger, https, m...|[nigger, https, m...|(10000,[0,1,12,11...|\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Times hate in test: 59102\n",
      "Times not hate in test: 180555\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Count: \" + str(hate_train_df.count()))\n",
    "hate_train_df.show(5)\n",
    "times_hate = hate_train_df.filter(hate_train_df['majority_label'] > 0.0).count()\n",
    "print(f'Times hate in training: {times_hate}')\n",
    "times_not_hate = hate_train_df.filter(hate_train_df['majority_label'] == 0.0).count()\n",
    "print(f'Times not hate in training: {times_not_hate}')\n",
    "\n",
    "print(\"Test Dataset Count: \" + str(hate_test_df.count()))\n",
    "hate_test_df.show(5)\n",
    "times_hate = hate_test_df.filter(hate_test_df['majority_label'] > 0.0).count()\n",
    "print(f'Times hate in test: {times_hate}')\n",
    "times_not_hate = hate_test_df.filter(hate_test_df['majority_label'] == 0.0).count()\n",
    "print(f'Times not hate in test: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.774575\n",
      "Times hate detected: 4314\n",
      "Times not hate detected: 19694\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# lr_classifier = LogisticRegression(family=\"multinomial\", labelCol=\"majority_label\", featuresCol=\"features\")\n",
    "\n",
    "# lr_classifier_pipeline = Pipeline(stages=[lr_classifier])\n",
    "# lr_trained_pipeline = lr_classifier_pipeline.fit(hate_train_df)\n",
    "# predictions = lr_trained_pipeline.transform(hate_test_df)\n",
    "\n",
    "# lr_model_evaluator = MulticlassClassificationEvaluator(\n",
    "#     labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# accuracy = lr_model_evaluator.evaluate(predictions)\n",
    "# print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "# times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "# print(f'Times hate detected: {times_hate}')\n",
    "# times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "# print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_trained_pipeline.write().overwrite().save(\"../models/model_lr\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "classifier = NaiveBayes(smoothing=1, labelCol=\"majority_label\", featuresCol=\"features\")\n",
    "\n",
    "classifier_pipeline = Pipeline(stages=[classifier])\n",
    "predictions = classifier_pipeline.fit(hate_train_df).transform(hate_test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.75724\n",
      "Times hate detected: 4691\n",
      "Times not hate detected: 19308\n"
     ]
    }
   ],
   "source": [
    "model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = model_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "print(f'Times hate detected: {times_hate}')\n",
    "times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_pipeline.write().overwrite().save(\"../models/model_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|              index|                text|majority_label|              tokens|            filtered|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|1024172939191754752|' she dyke energy...|             1|[she, dyke, energ...|[dyke, energy, ra...|(10000,[0,1,6,7,2...|[-35.057839778119...|[0.15343380004494...|       1.0|\n",
      "|1024173075607298048|called me he that...|             0|[called, me, he, ...|[called, day, wor...|(10000,[0,1,3,4,4...|[-47.258622461489...|[0.46748544250673...|       1.0|\n",
      "|1024228972417220608|calls this anyone...|             0|[calls, this, any...|[calls, anyone, a...|(10000,[0,1,3,361...|[-45.194900701406...|[0.47781451427140...|       1.0|\n",
      "|1024349305895677952|retarded https ae...|             1|[retarded, https,...|[retarded, https,...|(10000,[0,1,10,47...|[-21.874936181004...|[0.13206429426176...|       1.0|\n",
      "|1024407862737555456|meant better ✅ is...|             0|[meant, better, i...|[meant, better, g...|(10000,[0,1,17,11...|[-48.705907528434...|[0.20721492532764...|       1.0|\n",
      "+-------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['prediction'] == 1.0).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198584"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark_df_sample = spark_df #.sample(fraction=0.1) \n",
    "spark_df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_train_df, hate_test_df = spark_df_sample.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'We hate religion' > 'We' 'hate' 'religion'\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "# Remove stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Term frequency\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "# Classifier\n",
    "classifier = NaiveBayes(smoothing=1, labelCol=\"majority_label\", featuresCol=\"features\")\n",
    "\n",
    "inference_pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, classifier])\n",
    "trained_inference_pipeline = inference_pipeline.fit(hate_train_df)\n",
    "predictions = trained_inference_pipeline.transform(hate_test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save MLlib Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_inference_pipeline.write().overwrite().save(\"../models/mllib_model_nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.75724\n"
     ]
    }
   ],
   "source": [
    "model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"majority_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = model_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "\n",
    "# times_hate = predictions.filter(predictions['prediction'] == 1.0).count()\n",
    "# print(f'Times hate detected: {times_hate}')\n",
    "# times_not_hate = predictions.filter(predictions['prediction'] == 0.0).count()\n",
    "# print(f'Times not hate detected: {times_not_hate}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKlearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1198584 entries, 0 to 1198583\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   index           1198584 non-null  int64 \n",
      " 1   text            1198584 non-null  object\n",
      " 2   majority_label  1198584 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 27.4+ MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pandas_df = spark_df.toPandas()\n",
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_sample = pandas_df #.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pandas_df_sample['text'], pandas_df_sample['majority_label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english', min_df=2, max_features=10000)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "trained_inference_pipeline = inference_pipeline.fit(X_train, y_train)\n",
    "y_pred = trained_inference_pipeline.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/sklearn_model_nb.pkl']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(trained_inference_pipeline, '../models/sklearn_model_nb.pkl')\n",
    "# pipeline = joblib.load('pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7656849657934257"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "28d6d9e31a694f2aba28d42b1019d9365f56410d563150feaee59905aa4508a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
