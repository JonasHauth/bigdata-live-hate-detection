{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was lernen wir aus der Übung?\n",
    "- Mit dem Einsatz moderner Big Data-Technologien, wie MLLib, Spark, SparkStreaming und Parquet, konnten wir die Analyseprozesse effizienter und leistungsstärker gestalten\n",
    "- Dies ermöglicht eine schnelle Verarbeitung großer Datenmengen und erleichtert es uns, Erkenntnisse aus den Daten zu gewinnen.\n",
    "- Aber Big Data Ansätze sind nicht immer die besten, für \"kleine\" Probleme sind auch kleine Maschinen und einfache Anwendungen ausreichend. Man muss nicht immer mit Kanonenkugeln auf Spatzen schießen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was ist unser Eindruck zu Spark?\n",
    "- Spark ist sau Nice ( ͡° ͜ʖ ͡°)\n",
    "- Einfacher in der Handhabung als Map-Reduce.\n",
    "- Sehr viele verschiedene kompatible Anwendungen\n",
    "- Hohe Geschwindigkeit und Parallelität, Einfache API, Unterstützung für interaktive Anfragen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sollte man unseren Use Case mit Spark in der Realität angehen?\n",
    "Stark abhängig von dem konkreten Anwendungsfall:\n",
    "- Für Echtzeitanalysen Analysen mit Live Ergebnissen ist Spark hervorragend geeignet.\n",
    "- Spark hilft hier vor allem bei Skalierbarkeit, Fehlertoleranz und Performance.\n",
    "- Für Ex-Post Analysen ohne Echzeitanforderungen (z.B. für ein Reporting), würde auch \"klassisches\" Machine Learning ausreichen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wo sind die Chancen und Risiken?\n",
    "- Chancen: Schnelle Analyse großer Datenmengen, Skalierbarkeit, Hochverfügbarkeit\n",
    "- Risiken: Bisher eher schlechte Performance des Machine Learning Modells, Datenschutz/Privatsphäre der Twitter User, Was gilt als Hate und was nicht bzw. wer beurteilt das?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ist das wirklich ein Big Data Problem oder gäbe es einfachere Alternativen?\n",
    "- Unter der Voraussetzung, dass die Menge der Daten nicht zu groß wird (auf einer Festplatte gespeichert werden kann) und keine Echtzeitanforderung vorliegt könnten die Tweets auf einer Festplatte gespeichert und mit Hilfe klassischer ML Tools (NLP, RegEx) ausgewertet werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Würden wir einem Unternehmen raten bei einem solchen Probelm ein Spark Cluster anzuschaffen?\n",
    "Abhängig von verschiedenen Faktoren:\n",
    "- Größe der Datenmenge: Bei großen Datenmengen ist die Anschaffung eines Spark Clusters mit ausreichenden Rechenkapazitäten sinnvoll\n",
    "- Anforderungen an die Leistung (Live vs. Batch): Bestehen Echtzeitanforderungen bietet Spark mit einem ausreichend dimensionierten Cluster eine optimale Ausführungsumgebung.\n",
    "- Größe des Budget: Auch schon auf kleinen Clustern bzw. lokalen Spark Instanzen können viele Berechnungen durchgeführt werden (Insgesamt sind sowohl Spark als auch HDFS Open-Source und in der Anschaffung kostenfrei)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
